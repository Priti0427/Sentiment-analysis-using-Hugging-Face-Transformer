{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Install required packages (only needed once per environment).  \n",
    "Packages used:\n",
    "\n",
    "- `transformers` (Hugging Face) for pretrained models and pipelines\n",
    "- `torch` as backend for Transformer models\n",
    "- `pandas`, `numpy` for data wrangling\n",
    "- `scikit-learn` for metrics and train/test split\n",
    "- `matplotlib` for plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\danco\\anaconda3\\lib\\site-packages (23.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.8 MB 495.5 kB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.1/1.8 MB 1.1 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.2/1.8 MB 1.7 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.5/1.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.1/1.8 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 5.9 MB/s eta 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\Users\\danco\\anaconda3\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: numpy in c:\\users\\danco\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\danco\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\danco\\anaconda3\\lib\\site-packages (3.8.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\danco\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/44.0 kB ? eta -:--:--\n",
      "     ----------------- -------------------- 20.5/44.0 kB 330.3 kB/s eta 0:00:01\n",
      "     -------------------------------------- 44.0/44.0 kB 535.7 kB/s eta 0:00:00\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-2.9.0%2Bcpu-cp311-cp311-win_amd64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\danco\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\danco\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\danco\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\danco\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\danco\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\danco\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\danco\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\danco\\anaconda3\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\danco\\anaconda3\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\danco\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\danco\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\danco\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\danco\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\danco\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\danco\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\danco\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\danco\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\danco\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Collecting typing-extensions>=4.10.0 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\danco\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\danco\\anaconda3\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\danco\\anaconda3\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\danco\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\danco\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\danco\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\danco\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\danco\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\danco\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\danco\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\danco\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.10.5)\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/12.0 MB 1.7 MB/s eta 0:00:08\n",
      "    --------------------------------------- 0.2/12.0 MB 1.6 MB/s eta 0:00:08\n",
      "    --------------------------------------- 0.3/12.0 MB 1.8 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.4/12.0 MB 1.9 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.6/12.0 MB 2.2 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.7/12.0 MB 2.2 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.8/12.0 MB 2.3 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.0/12.0 MB 2.5 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.1/12.0 MB 2.5 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 1.3/12.0 MB 2.6 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 1.5/12.0 MB 2.7 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.7/12.0 MB 2.8 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.9/12.0 MB 2.9 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 2.1/12.0 MB 3.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 2.3/12.0 MB 3.1 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 2.5/12.0 MB 3.2 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.7/12.0 MB 3.3 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 3.0/12.0 MB 3.5 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.3/12.0 MB 3.5 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.5/12.0 MB 3.6 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 3.8/12.0 MB 3.8 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 4.2/12.0 MB 4.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.5/12.0 MB 4.1 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.9/12.0 MB 4.2 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.2/12.0 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.6/12.0 MB 4.5 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.1/12.0 MB 4.7 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.5/12.0 MB 4.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 7.0/12.0 MB 5.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.4/12.0 MB 5.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.9/12.0 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.4/12.0 MB 5.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.1/12.0 MB 5.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.6/12.0 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.3/12.0 MB 6.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.9/12.0 MB 7.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.6/12.0 MB 8.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 8.6 MB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/cpu/torch-2.9.0%2Bcpu-cp311-cp311-win_amd64.whl (109.2 MB)\n",
      "   ---------------------------------------- 0.0/109.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/109.2 MB 7.9 MB/s eta 0:00:14\n",
      "   ---------------------------------------- 0.6/109.2 MB 8.1 MB/s eta 0:00:14\n",
      "    --------------------------------------- 1.5/109.2 MB 11.0 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 3.2/109.2 MB 16.9 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 5.7/109.2 MB 24.4 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 8.5/109.2 MB 30.1 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 11.3/109.2 MB 46.7 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 14.3/109.2 MB 59.5 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 17.1/109.2 MB 59.5 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 20.0/109.2 MB 59.5 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 22.7/109.2 MB 65.6 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 25.6/109.2 MB 65.2 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 28.4/109.2 MB 59.5 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 29.6/109.2 MB 59.5 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 29.6/109.2 MB 59.5 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 34.6/109.2 MB 50.4 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 39.4/109.2 MB 59.5 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 42.2/109.2 MB 93.9 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 45.2/109.2 MB 81.8 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 48.1/109.2 MB 65.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 51.0/109.2 MB 65.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 53.7/109.2 MB 65.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 56.7/109.2 MB 65.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 56.7/109.2 MB 59.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 58.3/109.2 MB 46.7 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 63.5/109.2 MB 54.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 63.5/109.2 MB 54.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 67.7/109.2 MB 72.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 69.7/109.2 MB 65.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 73.1/109.2 MB 59.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 77.0/109.2 MB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 77.0/109.2 MB 73.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 82.4/109.2 MB 65.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 83.8/109.2 MB 65.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 86.0/109.2 MB 59.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 88.5/109.2 MB 59.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 91.9/109.2 MB 59.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 94.6/109.2 MB 59.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 98.0/109.2 MB 73.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 101.3/109.2 MB 65.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 105.9/109.2 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  109.2/109.2 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  109.2/109.2 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  109.2/109.2 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------- 109.2/109.2 MB 40.9 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
      "   ---------------------------------------- 0.0/564.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 564.3/564.3 kB 34.6 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "   ---------------------------------------- 0.0/320.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 320.2/320.2 kB 19.4 MB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 3.2/6.3 MB 67.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.3 MB 64.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 57.7 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.7/2.7 MB 21.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.7/2.7 MB 7.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.3/2.7 MB 9.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.2/2.7 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.7/2.7 MB 12.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 11.4 MB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "   ---------------------------------------- 0.0/44.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 44.6/44.6 kB 2.1 MB/s eta 0:00:00\n",
      "Installing collected packages: typing-extensions, sympy, safetensors, torch, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12\n",
      "    Uninstalling sympy-1.12:\n",
      "      Successfully uninstalled sympy-1.12\n",
      "Successfully installed huggingface-hub-0.35.3 safetensors-0.6.2 sympy-1.14.0 tokenizers-0.22.1 torch-2.9.0+cpu transformers-4.57.1 typing-extensions-4.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install numpy pandas matplotlib scikit-learn transformers torch --extra-index-url https://download.pytorch.org/whl/cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, math, random, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Path to  dataset file\n",
    "DATA_PATH = \"archive (5)/smile-annotations-final.csv\"\n",
    "\n",
    "print(\"Using data file:\", DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data & Quick EDA\n",
    "\n",
    "Load the SMILE dataset, show basic info, and preview a few rows to understand columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"\\nColumns:\", list(df.columns))\n",
    "print(\"\\nHead:\")\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Attempt to auto-detect text and label columns \n",
    "possible_text_cols = [c for c in df.columns if re.search(r\"(text|tweet|content|sentence)\", c, re.I)]\n",
    "possible_label_cols = [c for c in df.columns if re.search(r\"(label|sentiment|emotion|class|category)\", c, re.I)]\n",
    "\n",
    "text_col  = possible_text_cols[0] if possible_text_cols else df.columns[0]\n",
    "label_col = possible_label_cols[0] if possible_label_cols else df.columns[1]\n",
    "\n",
    "print(f\"\\nAuto-detected text column: {text_col}\")\n",
    "print(f\"Auto-detected label column: {label_col}\")\n",
    "\n",
    "# Drop NAs and keep only necessary columns\n",
    "df = df[[text_col, label_col]].dropna().rename(columns={text_col:\"text\", label_col:\"label\"}).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nUnique labels:\", sorted(df['label'].astype(str).unique().tolist()))\n",
    "print(\"\\nSample rows after renaming:\")\n",
    "display(df.sample(5, random_state=RANDOM_SEED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess: Clean Text & Map Emotions → Sentiment\n",
    "\n",
    "Clean the text (lowercase, strip URLs/mentions/hashtags), then **map fine-grained emotions** to **positive/negative/neutral**.  \n",
    "If the dataset already has `positive/negative/neutral`, the mapping will keep them as-is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_RE = re.compile(r\"http\\S+|www\\.\\S+\")\n",
    "MENTION_RE = re.compile(r\"@\\w+\")\n",
    "HASHTAG_RE = re.compile(r\"#(\\w+)\")\n",
    "MULTISPACE_RE = re.compile(r\"\\s+\")\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    s = URL_RE.sub(\"\", s)\n",
    "    s = MENTION_RE.sub(\"\", s)\n",
    "    s = HASHTAG_RE.sub(r\"\\1\", s)  # keep hashtag word\n",
    "    s = re.sub(r\"[^\\x00-\\x7F]+\", \" \", s)  # drop non-ascii (quick/simple)\n",
    "    s = MULTISPACE_RE.sub(\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].astype(str).apply(clean_text)\n",
    "\n",
    "# Define a flexible mapping from common SMILE-like emotion labels\n",
    "positive_set = {\"happy\",\"happiness\",\"joy\",\"love\",\"fun\",\"optimism\",\"surprise\",\"excited\",\"enthusiasm\",\"positive\"}\n",
    "negative_set = {\"anger\",\"angry\",\"hate\",\"worry\",\"sad\",\"sadness\",\"boredom\",\"annoyance\",\"disgust\",\"fear\",\"negative\"}\n",
    "neutral_set  = {\"neutral\",\"other\",\"none\",\"unknown\"}\n",
    "\n",
    "def map_to_sentiment(lbl: str) -> str:\n",
    "    l = str(lbl).strip().lower()\n",
    "    if l in positive_set: return \"POSITIVE\"\n",
    "    if l in negative_set: return \"NEGATIVE\"\n",
    "    if l in neutral_set:  return \"NEUTRAL\"\n",
    "    # Heuristic fallbacks (try to detect word parts)\n",
    "    if any(p in l for p in [\"happy\",\"joy\",\"love\",\"optim\"]): return \"POSITIVE\"\n",
    "    if any(n in l for n in [\"ang\",\"sad\",\"hate\",\"worr\",\"disgust\",\"fear\"]): return \"NEGATIVE\"\n",
    "    if \"neutral\" in l: return \"NEUTRAL\"\n",
    "    return \"NEUTRAL\"  # default safe fallback\n",
    "\n",
    "df[\"target\"] = df[\"label\"].astype(str).apply(map_to_sentiment)\n",
    "\n",
    "print(df[[\"text\",\"label\",\"clean_text\",\"target\"]].head(8))\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(df[\"target\"].value_counts(normalize=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test Split\n",
    "\n",
    "Create a stratified train/test split so the label proportions are similar across splits.  \n",
    "We keep only the cleaned text and mapped target for modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"clean_text\"].values\n",
    "Y = df[\"target\"].values\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=RANDOM_SEED, stratify=Y\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_train), \"| Test size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Two Sentiment Pipelines\n",
    "\n",
    "Build two Hugging Face `pipeline` objects.\n",
    "\n",
    "- Model A: `distilbert-base-uncased-finetuned-sst-2-english` (binary: POSITIVE/NEGATIVE).\n",
    "- Model B: `cardiffnlp/twitter-roberta-base-sentiment` (ternary: NEGATIVE/NEUTRAL/POSITIVE).  \n",
    "  We will post-process so both return one of {NEGATIVE, NEUTRAL, POSITIVE}.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a_id = \"distilbert-base-uncased-finetuned-sst-2-english\"  # binary\n",
    "model_b_id = \"cardiffnlp/twitter-roberta-base-sentiment\"        # ternary\n",
    "\n",
    "clf_a = pipeline(\"sentiment-analysis\", model=model_a_id, top_k=None)  # returns label/score\n",
    "clf_b = pipeline(\"sentiment-analysis\", model=model_b_id, top_k=None)\n",
    "\n",
    "def normalize_label(label: str) -> str:\n",
    "    l = label.upper()\n",
    "    if \"POS\" in l: return \"POSITIVE\"\n",
    "    if \"NEG\" in l: return \"NEGATIVE\"\n",
    "    if \"NEU\" in l: return \"NEUTRAL\"\n",
    "    # fallback for models without NEUTRAL: use sign\n",
    "    return l\n",
    "\n",
    "print(\"Pipelines ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batched Prediction Helper\n",
    "\n",
    "Define a **batched prediction** function that avoids memory spikes.  \n",
    "It returns predicted labels and the model's confidence scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "def predict_with_pipeline(texts: List[str], pipe, batch_size: int = 32) -> Tuple[List[str], List[float]]:\n",
    "    preds, confs = [], []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        out = pipe(batch)\n",
    "        for item in out:\n",
    "            # Some pipelines return list of dicts or dict; handle both\n",
    "            if isinstance(item, list) and len(item) > 0:\n",
    "                item = max(item, key=lambda d: d.get(\"score\", 0.0))\n",
    "            label = normalize_label(item[\"label\"])\n",
    "            score = float(item[\"score\"])\n",
    "            preds.append(label)\n",
    "            confs.append(score)\n",
    "    return preds, confs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate One Model\n",
    "\n",
    "Evaluate **one model** on the test set: accuracy, macro-F1, confusion matrix, and a short classification report.  \n",
    "We also visualize **prediction confidence** and **predicted label distribution**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name: str, pipe):\n",
    "    Y_pred, conf = predict_with_pipeline(list(X_test), pipe)\n",
    "    acc = accuracy_score(Y_test, Y_pred)\n",
    "    f1m = f1_score(Y_test, Y_pred, average=\"macro\")\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Macro-F1: {f1m:.4f}\\n\")\n",
    "    print(classification_report(Y_test, Y_pred, digits=4))\n",
    "    print(\"Confusion matrix (rows=true, cols=pred):\\n\", confusion_matrix(Y_test, Y_pred, labels=[\"NEGATIVE\",\"NEUTRAL\",\"POSITIVE\"]))\n",
    "\n",
    "    # Confidence histogram\n",
    "    plt.figure()\n",
    "    plt.hist(conf, bins=20)\n",
    "    plt.title(f\"{name} — Prediction Confidence\")\n",
    "    plt.xlabel(\"confidence\")\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.show()\n",
    "\n",
    "    # Predicted distribution\n",
    "    pd.Series(Y_pred).value_counts().plot(kind=\"bar\")\n",
    "    plt.title(f\"{name} — Predicted label distribution\")\n",
    "    plt.xlabel(\"label\")\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.show()\n",
    "\n",
    "    return {\"name\": name, \"acc\": acc, \"f1_macro\": f1m}\n",
    "\n",
    "res_a = evaluate_model(\"Model A — DistilBERT (SST-2)\", clf_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Second Model & Compare\n",
    "\n",
    "**Comment for this Markdown cell:** Evaluate the **second model** and print a compact comparison table so you can see which one performs better on this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_b = evaluate_model(\"Model B — Twitter RoBERTa (3-class)\", clf_b)\n",
    "\n",
    "comp = pd.DataFrame([res_a, res_b]).sort_values(\"f1_macro\", ascending=False).reset_index(drop=True)\n",
    "print(\"\\nComparison (higher is better):\")\n",
    "display(comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reusable Prediction Function + Simple Reasoning\n",
    "\n",
    "**Comment for this Markdown cell:** Provide a **reusable function** to predict on **any list of texts** with either model.  \n",
    "For **reasoning**: we print the model confidence and highlight whether the text contains simple **sentiment cue words** (a tiny lexicon). This is not true model explainability, but it offers an understandable rationale for beginners.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_CUES = {\"love\",\"great\",\"good\",\"happy\",\"joy\",\"win\",\"awesome\",\"wonderful\",\"amazing\",\"like\"}\n",
    "NEG_CUES = {\"hate\",\"bad\",\"sad\",\"angry\",\"annoyed\",\"worry\",\"worried\",\"terrible\",\"awful\",\"disgust\"}\n",
    "\n",
    "def explain_basic(text: str):\n",
    "    t = clean_text(text)\n",
    "    pos_hits = sorted({w for w in POS_CUES if re.search(rf\"\\\\b{re.escape(w)}\\\\b\", t)})\n",
    "    neg_hits = sorted({w for w in NEG_CUES if re.search(rf\"\\\\b{re.escape(w)}\\\\b\", t)})\n",
    "    return pos_hits, neg_hits\n",
    "\n",
    "def predict_texts(texts, model=\"a\"):\n",
    "    pipe = clf_a if str(model).lower().startswith(\"a\") else clf_b\n",
    "    preds, confs = predict_with_pipeline([clean_text(t) for t in texts], pipe)\n",
    "    rows = []\n",
    "    for text, label, conf in zip(texts, preds, confs):\n",
    "        pos_hits, neg_hits = explain_basic(text)\n",
    "        why = []\n",
    "        why.append(f\"model confidence={conf:.3f}\")\n",
    "        if pos_hits: why.append(f\"positive cues: {pos_hits}\")\n",
    "        if neg_hits: why.append(f\"negative cues: {neg_hits}\")\n",
    "        rows.append({\"text\": text, \"pred\": label, \"confidence\": conf, \"why_simple\": \"; \".join(why)})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Demo (feel free to edit)\n",
    "demo_df = predict_texts([\n",
    "    \"I love this new album so much!\",\n",
    "    \"This is bad and I'm really angry about it.\",\n",
    "    \"It's okay, nothing special.\"\n",
    "], model=\"b\")\n",
    "\n",
    "display(demo_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creative Application (Your Data!)\n",
    "\n",
    "**Comment for this Markdown cell:** Apply a chosen model to **your own domain** (e.g., song lyrics, news headlines).  \n",
    "Below are two examples:\n",
    "\n",
    "1. A **hardcoded list** (quick demo)\n",
    "2. Loading from a CSV with a column called `text` (edit path/column for your file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: hardcoded list\n",
    "my_domain_texts = [\n",
    "    \"Breaking: Markets rally as inflation cools to 2.5%.\",\n",
    "    \"Lyrics: I'm feeling good, like I should.\",\n",
    "    \"Match report: The team suffers a terrible defeat.\"\n",
    "]\n",
    "creative_results = predict_texts(my_domain_texts, model=\"b\")\n",
    "display(creative_results)\n",
    "\n",
    "# Example 2: from a CSV file you provide (uncomment and set your file path)\n",
    "# custom_df = pd.read_csv(\"/path/to/your_data.csv\")\n",
    "# creative_results2 = predict_texts(custom_df[\"text\"].astype(str).tolist(), model=\"b\")\n",
    "# display(creative_results2.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS: Fine-Tuning Template (Optional)\n",
    "\n",
    "**Comment for this Markdown cell:** (Optional) Template for **fine-tuning** a sentiment model with Hugging Face.  \n",
    "This is a **skeleton** — you must adapt it to your labels and ensure you have enough data.\n",
    "\n",
    "> Running this section can take time and requires a GPU for best results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare a Dataset object\n",
    "# train_df = pd.DataFrame({\"text\": X_train, \"label\": pd.Categorical(y_train).codes})\n",
    "# test_df  = pd.DataFrame({\"text\": X_test,  \"label\": pd.Categorical(y_test).codes})\n",
    "# label_names = list(pd.Categorical(df['target']).categories)\n",
    "# label2id = {name:i for i,name in enumerate(label_names)}\n",
    "# id2label = {i:name for name,i in label2id.items()}\n",
    "\n",
    "# ds_train = Dataset.from_pandas(train_df)\n",
    "# ds_test  = Dataset.from_pandas(test_df)\n",
    "\n",
    "# def tok_fn(batch):\n",
    "#     return tokenizer(batch[\"text\"], truncation=True)\n",
    "# ds_train = ds_train.map(tok_fn, batched=True)\n",
    "# ds_test  = ds_test.map(tok_fn, batched=True)\n",
    "\n",
    "# collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     base_model_id, num_labels=len(label_names), id2label=id2label, label2id=label2id\n",
    "# )\n",
    "\n",
    "# args = TrainingArguments(\n",
    "#     output_dir=\"./sentiment_ft\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     num_train_epochs=2,\n",
    "#     weight_decay=0.01,\n",
    "#     logging_steps=50\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=args,\n",
    "#     train_dataset=ds_train,\n",
    "#     eval_dataset=ds_test,\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=collator,\n",
    "# )\n",
    "\n",
    "# trainer.train()\n",
    "# trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "**What we accomplished:**\n",
    "\n",
    "- Built a complete **sentiment analysis** workflow with two pretrained models.\n",
    "- Evaluated with **accuracy, macro-F1**, and confusion matrices.\n",
    "- Visualized **confidence** and **label distributions**.\n",
    "- Created a reusable **prediction function** and applied it to a new domain.\n",
    "- (Bonus) Prepared a **fine-tuning template** for domain adaptation.\n",
    "\n",
    "**Next steps:**\n",
    "\n",
    "- Collect more domain-specific labeled data and try the fine-tuning section.\n",
    "- Explore more models in the Hugging Face Hub (larger RoBERTa/BERT variants).\n",
    "- Add true explainability (e.g., SHAP, LIME) once you are comfortable with the basics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
